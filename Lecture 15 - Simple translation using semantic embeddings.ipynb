{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 15 - Simple translation using semantic embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58LjICoY5YsV"
      },
      "source": [
        "# Simple translation using semantic embeddings\n",
        "\n",
        "Joulin et al. (2018) \"[*Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion*](https://arxiv.org/abs/1804.07745),\" arXiv\n",
        "\n",
        "The version of the fasttext embedding being downloaded is *aligned*. As such, the word for 'cat' in the English embedding will be close to the word 'katt' (Swedish) and 'gato' (Spanish) in their respective embeddings. You choose the language to load by giving the loader a language code. After a fasttext embedding has been downloaded, it will be cached on the local file system to reduce reloading time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1-78pxMAa_Q"
      },
      "source": [
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "from tqdm import tqdm\n",
        "import os.path\n",
        "import time\n",
        "\n",
        "class AlignedEmbedding(object):\n",
        "  \"\"\"This data loader will download an aligned fasttext embedding given a two letter language code.\n",
        "  More information ca be found at https://fasttext.cc/docs/en/aligned-vectors.html .\"\"\"\n",
        "  def __init__(self, lang='en', vocabulary_limit=-1):\n",
        "    \"\"\"Data loader\"\"\"\n",
        "    self._dataurl = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.%s.align.vec\" % lang\n",
        "    cachefile = \"cache_%s.npz\" % lang\n",
        "    if os.path.exists(cachefile):\n",
        "      print(\"Loading cached data from %s...\" % cachefile, end=\"\")\n",
        "      t = time.time()\n",
        "      data = np.load(cachefile)\n",
        "      self.idx2token_ = list(data['idx2token'])\n",
        "      self.vectors_ = data['vectors']\n",
        "      print(\"done (%isec)\" % (time.time()-t), flush=True)\n",
        "    else:\n",
        "      with urlopen(self._dataurl) as remote_file:\n",
        "        get_decoded_line = lambda file: file.readline().decode('utf-8')\n",
        "        self.n_tokens, self.n_dim = [int(n) for n in get_decoded_line(remote_file).split()]\n",
        "        if vocabulary_limit > 0:\n",
        "          self.n_tokens = vocabulary_limit\n",
        "        self.idx2token_ = list()\n",
        "        self.vectors_ = np.zeros((self.n_tokens, self.n_dim), dtype=np.float32)\n",
        "        for n in tqdm(range(self.vectors_.shape[0]), desc=\"Downloading and parsing vectors\", unit=\"words\"):\n",
        "          textline = get_decoded_line(remote_file)\n",
        "          linedata = textline.split(' ')\n",
        "          self.idx2token_.append(linedata[0])\n",
        "          self.vectors_[n, :] = np.asarray(linedata[1:], dtype=np.float32)\n",
        "      np.savez(cachefile, vectors=self.vectors_, idx2token=self.idx2token_)\n",
        "    self.token2idx_ = {token:i for i, token in enumerate(self.idx2token_)}\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Gives the number of tokens in the embedding.\"\"\"\n",
        "    return len(self.token2idx_)\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    \"\"\"Returns the vector(s) for a token or a list of tokens.\"\"\"\n",
        "    assert isinstance(tokens, (str, list)), \"tokens must be list or str.\"\n",
        "    if type(tokens) is str:\n",
        "      assert tokens in self, \"Cound not find token '%s'\" %s\n",
        "      return self.vectors_[self.token2idx_[tokens]].reshape(1, -1)\n",
        "    else:\n",
        "      ret = np.zeros((len(tokens), self.vectors_.shape[1]))\n",
        "      for i, token in enumerate(tokens):\n",
        "        ret[i, :] = self[token]\n",
        "      return ret\n",
        "\n",
        "  def __contains__(self, token):\n",
        "    \"\"\"Allows a user to query if a tokens is in the embedding.\"\"\"\n",
        "    return token in self.token2idx_.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxYE4pG9t0iT",
        "outputId": "0d7191f9-f230-4243-cf5d-8ad62fed0f0d"
      },
      "source": [
        "embedding1 = AlignedEmbedding('en')\n",
        "embedding2 = AlignedEmbedding('es')\n",
        "\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached data from cache_en.npz...done (143sec)\n",
            "Loading cached data from cache_es.npz...done (49sec)\n",
            "total 7009692\n",
            "-rw-r--r-- 1 root root 5320910082 Oct 18 12:22 cache_en.npz\n",
            "-rw-r--r-- 1 root root 1856997146 Oct 18 12:24 cache_es.npz\n",
            "drwxr-xr-x 1 root root       4096 Oct  8 13:45 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHRqFoEIVUKf"
      },
      "source": [
        "assert len(embedding1) > 10000\n",
        "assert 'man' in embedding1\n",
        "assert 'woman' in embedding1\n",
        "assert 'kdjf343' not in embedding1\n",
        "assert 'king' in embedding1\n",
        "\n",
        "assert embedding1['man'].shape[0] == 1\n",
        "assert embedding1['man'].shape[1] == 300\n",
        "assert embedding1[['man', 'woman']].shape[0] == 2\n",
        "assert embedding1[['man', 'woman']].shape[1] == 300\n",
        "assert type(embedding1['man']) is np.ndarray\n",
        "assert np.isclose(np.sum(embedding1['man']-embedding1['man']), 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-JvgiXoiJNc"
      },
      "source": [
        "## Word distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHYez21yUZG8",
        "outputId": "2ee9d86d-46b4-424e-d858-21117eeafb8f"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "\n",
        "word = 'blue'\n",
        "source = embedding1\n",
        "target = embedding2\n",
        "\n",
        "print(\"Query word: %s\" % word)\n",
        "print(\"Euclidean\\t\\t\\tCosine\")\n",
        "distances = euclidean_distances(source[word], \n",
        "                                target.vectors_).ravel()\n",
        "similarities = cosine_similarity(source[word], \n",
        "                                 target.vectors_).ravel()\n",
        "k = 20\n",
        "for i, j in zip(np.argsort(distances)[:k], \n",
        "                np.argsort(similarities)[::-1][:k]):\n",
        "  print(\"%.3f %s %.3f %s\" % (distances[i], target.idx2token_[i].ljust(25), \n",
        "                             similarities[j], target.idx2token_[j]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query word: blue\n",
            "Euclidean\t\t\tCosine\n",
            "0.959 azul                      0.540 azul\n",
            "0.964 blue                      0.535 blue\n",
            "1.015 amarillo                  0.485 amarillo\n",
            "1.035 rojo                      0.464 rojo\n",
            "1.036 azules                    0.463 azules\n",
            "1.054 azuli                     0.445 azuli\n",
            "1.054 amarillo/naranja          0.445 amarillo/naranja\n",
            "1.054 amarillo,                 0.444 amarillo,\n",
            "1.059 azul/verde                0.439 azul/verde\n",
            "1.065 azul/                     0.433 azul/\n",
            "1.068 amarillo/verde            0.430 amarillo/verde\n",
            "1.070 azul,                     0.428 azul,\n",
            "1.071 amarillo/blanco           0.426 amarillo/blanco\n",
            "1.075 verde                     0.422 verde\n",
            "1.079 azule                     0.418 azule\n",
            "1.080 amarillos                 0.417 amarillos\n",
            "1.083 color                     0.414 color\n",
            "1.084 azulamarillo              0.412 azulamarillo\n",
            "1.085 amarillo—                 0.412 amarillo—\n",
            "1.086 amarill                   0.410 amarill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX_7UbfKeLzH"
      },
      "source": [
        "## Word similarity\n",
        "\n",
        "We should be able to meassure similarity as cosine similarity.\n",
        "\n",
        "$cos(\\overrightarrow{cat}, \\overrightarrow{dog}) \\geq cos(\\overrightarrow{cat}, \\overrightarrow{cow})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwrYqvyYMUQq"
      },
      "source": [
        "assert cosine_similarity(embedding1['cat'], embedding1['dog']) > cosine_similarity(embedding1['cat'], embedding1['cow'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}