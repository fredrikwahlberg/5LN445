{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character n-gram models\n",
    "\n",
    "First, we need some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available books: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/fredrik/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from ngram import NGramModel\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "print(\"Available books:\", gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 models\n",
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...] \t 1-gram model with 7811 unique keys\n",
      "['[', 'Persuasion', 'by', 'Jane', 'Austen', '1818', ...] \t 1-gram model with 6132 unique keys\n",
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', ...] \t 1-gram model with 6833 unique keys\n"
     ]
    }
   ],
   "source": [
    "fileids = gutenberg.fileids()[:3]\n",
    "\n",
    "def make_unigram_models(fileid):\n",
    "    from nltk.corpus import gutenberg\n",
    "    return NGramModel(gutenberg.words(fileid), 1)\n",
    "models = list(map(make_unigram_models, fileids))\n",
    "\n",
    "print(\"Created %i models\" % len(models))\n",
    "\n",
    "for fid, m in zip(fileids, models):\n",
    "    print(gutenberg.words(fid), \"\\t\", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of all english words in the corpus will also come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 7079 english words so far\n",
      "We have 8824 english words so far\n",
      "We have 10294 english words so far\n"
     ]
    }
   ],
   "source": [
    "def word_isalpha(word):\n",
    "    for c in word:\n",
    "        if not c.isalpha():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "english_words = set()\n",
    "for m in models:\n",
    "    words = [k[0].lower() for k in list(m.keys())]\n",
    "    english_words.update(set([w for w in words if word_isalpha(w)]))\n",
    "    print(\"We have %i english words so far\" % len(english_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['undressed', 'congratulatory', 'expedient', 'stilton', 'admire', 'ought', 'escorted', 'eight', 'serviceable', 'gallantry', 'favourites', 'knightley', 'untowardly', 'properer', 'swiftly', 'established', 'provide', 'acknowledge', 'premeditated', 'drove', 'proving', 'conundrums', 'disappearing', 'volumes', 'reliance', 'squire', 'song', 'substantial', 'certain', 'suspecting', 'delicately', 'tis', 'crosser', 'acts', 'minutes', 'compliments', 'wedging', 'implicit', 'defies', 'newspaper', 'blunders', 'counter', 'view', 'captivating', 'varied', 'dr', 'variance', 'lavish', 'livings', 'incurring', 'marking', 'enjoy', 'consulting', 'wronged', 'memorandum', 'stammered', 'commonplace', 'peculiar', 'apart', 'courtland', 'obligations', 'collar', 'marmion', 'unwillingness', 'relating', 'madam', 'sharing', 'afforded', 'vacancies', 'company', 'measure', 'incurring', 'nash', 'passed', 'behaved', 'headaches', 'mine', 'mistake', 'medals', 'hasty', 'warmly', 'tyrannic', 'regulate', 'sharer', 'returned', 'top', 'luckily', 'silversmith', 'if', 'hesitate', 'bitter', 'aching', 'exactly', 'warehouse', 'stipulate', 'horribly', 'illegitimacy', 'strip', 'beat', 'repressing']\n"
     ]
    }
   ],
   "source": [
    "from random import choices\n",
    "print(choices(list(english_words), k=100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up some Austen books as training data for character models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had b\n",
      "---\n",
      "emma by jane austen   volume i  chapter i   emma woodhouse handsome clever and rich with a comfortable home and happy disposition seemed to unite some of the best blessings of existence and had lived nearly twentyone years in the world with very little to distress or vex her  she was the youngest of the two daughters of a most affectionate indulgent father and had in consequence of her sisters marriage been mistress of his house from a very early period  her mother had died too long ago for her to have more than an indistinct remembrance of her caresses and her place had been supplied by an ex\n",
      "1948591 characters in training data\n"
     ]
    }
   ],
   "source": [
    "austen_text = [gutenberg.raw(fid) for fid in ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt']]\n",
    "print(austen_text[0][:600])\n",
    "\n",
    "def generate_alphabet(alpha, omega):\n",
    "    \"\"\"Set of the english alphabet\"\"\"\n",
    "    return set([chr(i) for i in range(ord(alpha), ord(omega)+1)]) \n",
    "\n",
    "def clean_text(text, allowed):\n",
    "    ret = text.lower()\n",
    "    strip = set(ret).difference(allowed)\n",
    "    if \" \" in allowed:\n",
    "        for s in strip:\n",
    "            if s in ['\\n', '\\t']:\n",
    "                ret = ret.replace(s, \" \")\n",
    "            else:\n",
    "                ret = ret.replace(s, \"\")\n",
    "    else:\n",
    "        for s in strip:\n",
    "            ret = ret.replace(s, \"\")\n",
    "    return ret\n",
    "\n",
    "alphabet = generate_alphabet('a', 'z') # Set of the english alphabet\n",
    "allowed_characters = set([' '])\n",
    "allowed_characters.update(alphabet)\n",
    "for i in range(len(austen_text)):\n",
    "    austen_text[i] = clean_text(austen_text[i], allowed_characters)\n",
    "\n",
    "print(\"---\")\n",
    "print(austen_text[0][:600])\n",
    "print(\"%i characters in training data\" % np.sum([len(t) for t in austen_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data cleaned, we are ready to create some character ngram models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reduce() arg 2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1b59eb2c6da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0munigram_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mausten_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: reduce() arg 2 must support iteration"
     ]
    }
   ],
   "source": [
    "#spark_texts = spark.sparkContext.parallelize(austen_text)\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool() as p:\n",
    "unigram_model = list(map(lambda data: NGramModel(list(data), 1), austen_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-abe8e9958023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mausten_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0munigram_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbigram_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "print(unigram_model)\n",
    "bigram_model = spark_texts.map(lambda data: NGramModel(list(data), 2)).reduce(lambda a, b: a.union(b))\n",
    "print(bigram_model)\n",
    "trigram_model = spark_texts.map(lambda data: NGramModel(list(data), 3)).reduce(lambda a, b: a.union(b))\n",
    "print(trigram_model)\n",
    "quadgram_model = spark_texts.map(lambda data: NGramModel(list(data), 4)).reduce(lambda a, b: a.union(b))\n",
    "print(quadgram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unigram:\", \"\".join(unigram_model.predict_sequence(90)))\n",
    "print()\n",
    "print(\"bigram:\", \"\".join(bigram_model.predict_sequence(90)))\n",
    "print()\n",
    "print(\"trigram:\", \"\".join(trigram_model.predict_sequence(90)))\n",
    "print()\n",
    "print(\"quadgram:\", \"\".join(trigram_model.predict_sequence(90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the most common characters in this english text and their probabilities (from relative frequencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram import ordered_ngrams\n",
    "for unigram, prob in list(ordered_ngrams(unigram_model)):\n",
    "    print(\"%s - %.5f\" % (unigram, prob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
